
import numpy as np
from colorama import Fore, Style
from PIL import Image
from io import BytesIO
import base64
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import os

from coffeedd.ml_logic.data import create_dataset_from_directory, create_tf_dataset
from coffeedd.ml_logic.custom_weights import get_class_weights
from coffeedd.ml_logic.registry_ml import load_model, save_results, save_model, mlflow_transition_model, mlflow_run
from coffeedd.ml_logic.model import initialize_model, compile_model, train_model
from coffeedd.ml_logic.data_analysis import plot_training_metrics_combined, analyze_training_convergence_combined, analyze_false_negatives, analyze_disease_recall
from coffeedd.ml_logic.gcs_upload import upload_latest_model_to_gcs, list_models_in_gcs
from coffeedd.params import LOCAL_DATA_PATH, CLASS_NAMES, SAMPLE_SIZE, BATCH_SIZE, FINE_TUNE, SAMPLE_NAME, MODEL_TARGET, MODELS_PATH, NUM_CLASSES, MODEL_NAME, PRODUCTION_MODEL

# Sistema de cach√© global para el modelo
_cached_model = None
_cached_model_path = None
_production_model_cache = None  # Cach√© espec√≠fico para modelo de producci√≥n

def get_cached_model():
    """
    Obtiene el modelo desde cach√© o lo carga si no est√° en memoria.

    Estrategia de carga:
    1. Si PRODUCTION_MODEL est√° definido, usa ese modelo espec√≠fico (r√°pido)
    2. Si no, busca el modelo m√°s reciente (normal)
    3. Para GCS: evita b√∫squedas innecesarias cuando hay modelo en producci√≥n

    Returns:
        Model: Modelo Keras cargado o None si no existe
    """
    global _cached_model, _cached_model_path, _production_model_cache

    try:
        # Verificar si hay un modelo espec√≠fico de producci√≥n configurado
        production_model = PRODUCTION_MODEL

        if production_model:
            return _get_production_model(production_model)
        else:
            return _get_latest_model()

    except Exception as e:
        print(Fore.RED + f"‚ùå Error en get_cached_model(): {e}" + Style.RESET_ALL)
        # En caso de error, intentar carga directa
        return load_model()

def _get_production_model(production_model_name):
    """
    Carga un modelo espec√≠fico de producci√≥n (r√°pido, sin b√∫squedas)

    Args:
        production_model_name: Nombre del modelo (ej: model_VGG16_20251102-073551.keras)
    """
    global _production_model_cache

    from coffeedd.params import LOCAL_REGISTRY_PATH, MODEL_ARCHITECTURE
    import os

    # Si ya tenemos el modelo de producci√≥n en cach√©
    if _production_model_cache is not None and _production_model_cache.get('name') == production_model_name:
        print(Fore.GREEN + f"‚ö° Usando modelo de producci√≥n desde cach√©: {production_model_name}" + Style.RESET_ALL)
        return _production_model_cache['model']

    # Buscar el modelo espec√≠fico localmente
    models_base_dir = os.path.join(LOCAL_REGISTRY_PATH, "models")
    architecture_dir = os.path.join(models_base_dir, MODEL_ARCHITECTURE.lower())
    production_model_path = os.path.join(architecture_dir, production_model_name)

    # Si existe localmente, usarlo
    if os.path.exists(production_model_path):
        print(Fore.CYAN + f"üéØ Cargando modelo de producci√≥n: {production_model_name}" + Style.RESET_ALL)
        print(Fore.BLUE + f"üìÅ Desde: {production_model_path}" + Style.RESET_ALL)

        # Cargar directamente sin buscar otros modelos
        from coffeedd.ml_logic.registry_ml import load_specific_model
        model = load_specific_model(production_model_path)

        if model:
            # Almacenar en cach√© de producci√≥n
            _production_model_cache = {
                'name': production_model_name,
                'path': production_model_path,
                'model': model
            }
            print(Fore.GREEN + f"‚úÖ Modelo de producci√≥n cargado y cacheado" + Style.RESET_ALL)
            return model

    # Si no existe localmente y MODEL_TARGET=gcs, intentar descargarlo
    if MODEL_TARGET == "gcs":
        print(Fore.YELLOW + f"‚ö†Ô∏è  Modelo de producci√≥n no encontrado localmente: {production_model_name}" + Style.RESET_ALL)
        print(Fore.BLUE + f"üîç Buscando en GCS..." + Style.RESET_ALL)

        # Buscar espec√≠ficamente este modelo en GCS
        model = _download_specific_model_from_gcs(production_model_name)
        if model:
            _production_model_cache = {
                'name': production_model_name,
                'path': f"gcs:{production_model_name}",
                'model': model
            }
            return model

    # Si no se encuentra el modelo de producci√≥n, fallback al m√©todo normal
    print(Fore.YELLOW + f"‚ö†Ô∏è  Modelo de producci√≥n '{production_model_name}' no encontrado" + Style.RESET_ALL)
    print(Fore.BLUE + f"üîÑ Fallback: buscando √∫ltimo modelo disponible..." + Style.RESET_ALL)
    return _get_latest_model()

def _get_latest_model():
    """
    Obtiene el modelo m√°s reciente (m√©todo original)
    """
    global _cached_model, _cached_model_path

    from coffeedd.ml_logic.registry_ml import find_latest_model_by_architecture
    from coffeedd.params import LOCAL_REGISTRY_PATH, MODEL_ARCHITECTURE
    import os

    models_base_dir = os.path.join(LOCAL_REGISTRY_PATH, "models")
    latest_model_path = find_latest_model_by_architecture(models_base_dir, MODEL_ARCHITECTURE.lower())

    # Si no hay modelo en disco, verificar GCS si es el target
    if latest_model_path is None and MODEL_TARGET == "gcs":
        print(Fore.BLUE + "üîç Verificando modelo en GCS..." + Style.RESET_ALL)
        # Forzar carga desde GCS (esto actualizar√° el cach√© local)
        _cached_model = load_model()
        _cached_model_path = "gcs_loaded"  # Marcar como cargado desde GCS
        return _cached_model

    # Si no hay modelo disponible
    if latest_model_path is None:
        print(Fore.YELLOW + "‚ö†Ô∏è  No hay modelo disponible" + Style.RESET_ALL)
        _cached_model = None
        _cached_model_path = None
        return None

    # Si ya tenemos el modelo en cach√© y la ruta no ha cambiado
    if _cached_model is not None and _cached_model_path == latest_model_path:
        print(Fore.GREEN + "‚ö° Usando modelo desde cach√© en memoria" + Style.RESET_ALL)
        return _cached_model

    # Si hay un modelo m√°s reciente o no tenemos cach√©
    if _cached_model_path != latest_model_path:
        if _cached_model_path is not None:
            print(Fore.BLUE + "üîÑ Modelo actualizado detectado, recargando cach√©..." + Style.RESET_ALL)
        else:
            print(Fore.BLUE + "üîÑ Cargando modelo en cach√©..." + Style.RESET_ALL)

        # Cargar el modelo
        _cached_model = load_model()
        _cached_model_path = latest_model_path

        if _cached_model is not None:
            print(Fore.GREEN + "‚úÖ Modelo cargado y almacenado en cach√©" + Style.RESET_ALL)

        return _cached_model

    return _cached_model

def _download_specific_model_from_gcs(model_name):
    """
    Descarga un modelo espec√≠fico desde GCS sin buscar todos los modelos

    Args:
        model_name: Nombre del modelo a descargar
    """
    try:
        from google.cloud import storage
        from coffeedd.params import BUCKET_NAME, LOCAL_REGISTRY_PATH, MODEL_ARCHITECTURE
        import os

        client = storage.Client()
        bucket = client.bucket(BUCKET_NAME)

        # Construir la ruta en GCS
        gcs_path = f"models/{MODEL_ARCHITECTURE.lower()}/{model_name}"
        blob = bucket.blob(gcs_path)

        if not blob.exists():
            print(Fore.RED + f"‚ùå Modelo no encontrado en GCS: {gcs_path}" + Style.RESET_ALL)
            return None

        # Crear directorio local
        architecture_dir = os.path.join(LOCAL_REGISTRY_PATH, "models", MODEL_ARCHITECTURE.lower())
        os.makedirs(architecture_dir, exist_ok=True)

        # Descargar
        local_path = os.path.join(architecture_dir, model_name)
        print(Fore.BLUE + f"üì• Descargando modelo espec√≠fico: {gcs_path}" + Style.RESET_ALL)
        blob.download_to_filename(local_path)

        # Cargar el modelo descargado
        from coffeedd.ml_logic.registry_ml import load_specific_model
        model = load_specific_model(local_path)

        if model:
            print(Fore.GREEN + f"‚úÖ Modelo de producci√≥n descargado y cargado" + Style.RESET_ALL)

        return model

    except Exception as e:
        print(Fore.RED + f"‚ùå Error descargando modelo espec√≠fico: {e}" + Style.RESET_ALL)
        return None

def clear_model_cache():
    """Limpia el cach√© del modelo (√∫til despu√©s de entrenar un nuevo modelo)"""
    global _cached_model, _cached_model_path
    _cached_model = None
    _cached_model_path = None
    print(Fore.BLUE + "üßπ Cach√© del modelo limpiado" + Style.RESET_ALL)

def warm_model_cache():
    """Precalienta el cach√© cargando el modelo en memoria"""
    print(Fore.CYAN + "üî• Precalentando cach√© del modelo..." + Style.RESET_ALL)
    model = get_cached_model()
    if model is not None:
        print(Fore.GREEN + "‚úÖ Cach√© precalentado exitosamente" + Style.RESET_ALL)
        return True
    else:
        print(Fore.YELLOW + "‚ö†Ô∏è  No hay modelo disponible para precalentar" + Style.RESET_ALL)
        return False

def get_cache_status():
    """Obtiene el estado actual del cach√©"""
    global _cached_model, _cached_model_path

    model_architecture = None
    if _cached_model:
        try:
            from coffeedd.ml_logic.registry_ml import detect_model_architecture
            model_architecture = detect_model_architecture(_cached_model.layers)
        except:
            model_architecture = "unknown"

    status = {
        "has_cached_model": _cached_model is not None,
        "cached_model_path": _cached_model_path,
        "model_layers": len(_cached_model.layers) if _cached_model else None,
        "model_architecture": model_architecture
    }

    return status

@mlflow_run
def train(metrics_viz=True, test_mode=False):
    print(Fore.MAGENTA + "\n‚≠êÔ∏è Empezando el entrenamiento del modelo... ‚≠êÔ∏è" + Style.RESET_ALL)

    print(f"üöÄ Cargando datos... con {SAMPLE_NAME}")
    (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = \
        create_dataset_from_directory(LOCAL_DATA_PATH, CLASS_NAMES, sample_size=SAMPLE_SIZE)

    # Crear datasets de TensorFlow
    train_dataset = create_tf_dataset(train_paths, train_labels, BATCH_SIZE,
                                    is_training=True, augment=True)
    val_dataset = create_tf_dataset(val_paths, val_labels, BATCH_SIZE,
                                    is_training=False, augment=False)
    test_dataset = create_tf_dataset(test_paths, test_labels, BATCH_SIZE,
                                    is_training=False, augment=False)

    print("\n‚úÖ Datasets creados exitosamente (carga on-the-fly activada)")

    class_weights = get_class_weights(
        train_labels=train_labels,
    )

    #Entrenar el modelo usando `model.py`
        # Cargar o inicializar modelo
    try:
        model = get_cached_model()
        if model is not None:
            print(Fore.GREEN + "‚úÖ Modelo existente cargado exitosamente" + Style.RESET_ALL)
        else:
            print(Fore.YELLOW + "‚ö†Ô∏è  No hay modelo existente, creando uno nuevo..." + Style.RESET_ALL)
            model = initialize_model(train_labels)
    except Exception as e:
        print(Fore.YELLOW + f"‚ö†Ô∏è  Error al cargar modelo existente: {str(e)[:100]}" + Style.RESET_ALL)
        print(Fore.BLUE + "üîÑ Creando modelo nuevo..." + Style.RESET_ALL)
        model = initialize_model(train_labels)

    model = compile_model(model)

    model, history = train_model(
        model,
        train_dataset,
        train_labels,
        val_dataset,
        val_labels,
        class_weights=class_weights,
        fine_tune=FINE_TUNE
    )

    val_recall = np.max(history.history['val_recall'])
    print(Fore.GREEN + f"\n‚úÖ Entrenamiento completado. Mejor recall en validaci√≥n: {val_recall:.4f}" + Style.RESET_ALL)

    val_disease_recall = np.max(history.history['val_disease_recall'])
    print(Fore.GREEN + f"\n‚úÖ Entrenamiento completado. Mejor disease recall en validaci√≥n: {val_disease_recall:.4f}" + Style.RESET_ALL)

    # ==========================================
    # NUEVO: VISUALIZACIONES DE M√âTRICAS DE ENTRENAMIENTO (HISTORIAL COMBINADO)
    # ==========================================
    if metrics_viz:
        training_viz_metrics = {}
        convergence_metrics = {}

        if history is not None:
            print(f"\n{Fore.MAGENTA}üìä Generando visualizaciones de entrenamiento...{Style.RESET_ALL}")

            model_name = MODEL_NAME

            # Generar visualizaciones de m√©tricas de entrenamiento
            training_viz_metrics = plot_training_metrics_combined(
                combined_history=history,
                model_name=model_name,
                sample_name=SAMPLE_NAME,
                test_labels=test_labels,
                y_pred_test_classes=None,
                verbose=True
            )

            # An√°lisis de convergencia del entrenamiento
            convergence_metrics = analyze_training_convergence_combined(
                combined_history=history,
                verbose=True
            )
            print(f"     - Visualizaci√≥n entrenamiento: {len(training_viz_metrics)}")
            print(f"     - Convergencia: {len(convergence_metrics)}")

            # Informaci√≥n sobre archivos generados
            files_generated = []

            if training_viz_metrics:
                training_metrics_file = f'{MODELS_PATH}/training_metrics_{model_name}.png'
                files_generated.append(training_metrics_file)

            print(f"   ‚Ä¢ Archivos generados: {len(files_generated)}")
            for i, file_path in enumerate(files_generated, 1):
                print(f"     {i}. {file_path}")


        else:
            print(f"\n{Fore.YELLOW}‚ö†Ô∏è  No se proporcion√≥ historial de entrenamiento. Saltando visualizaciones.{Style.RESET_ALL}")


    params = dict(
        context="train" if not test_mode else "test_train",
        training_set_size=SAMPLE_NAME,
        img_count=len(train_labels),
        model_name=model_name,
        fine_tune=FINE_TUNE
    )

    # Guardar resultados y modelo entrenado si no es modo test
    if not test_mode:
        # Combinar m√©tricas manejando duplicados (convergence_metrics tiene prioridad)
        combined_metrics = training_viz_metrics.copy()
        combined_metrics.update(convergence_metrics)

        # Save results on the hard drive using coffeedd.ml_logic.registry
        save_results(params=params, metrics=combined_metrics)

        # Save model weight on the hard drive (and optionally on GCS too!)
        save_model(model=model)

        # Limpiar cach√© despu√©s de guardar nuevo modelo
        clear_model_cache()

        # El ultimo modelo debe ser movido a "Staging" en MLflow si se usa MLflow
        if MODEL_TARGET == "mlflow":
            mlflow_transition_model(current_stage="None", new_stage="Staging")

    print(Fore.MAGENTA + "\nüèÅ Proceso de entrenamiento finalizado. üèÅ" + Style.RESET_ALL)

    return {"val_recall":val_recall, "val_disease_recall":val_disease_recall}

@mlflow_run
def evaluate(confusion_matrix_viz=True, false_negatives_analysis=True):
    """
    Eval√∫a el modelo entrenado en el test set y genera m√©tricas detalladas

    Args:
        combined_history: History combinado de train_model() (opcional)
    """
    print(Fore.MAGENTA + "\nüß™ Empezando evaluaci√≥n del modelo... üß™" + Style.RESET_ALL)

    # Cargar el modelo entrenado
    model = get_cached_model()
    assert model is not None, "Modelo no encontrado. Entrena el modelo primero."

    print(f"üöÄ Cargando datos de test... con {SAMPLE_NAME}")

    # Cargar datasets (solo necesitamos test)
    (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = \
        create_dataset_from_directory(LOCAL_DATA_PATH, CLASS_NAMES, sample_size=SAMPLE_SIZE)

    # Crear dataset de test
    test_dataset = create_tf_dataset(test_paths, test_labels, BATCH_SIZE,
                                   is_training=False, augment=False)

    print("\n‚úÖ Dataset de test creado exitosamente")

    # Constantes
    model_name = MODEL_NAME
    sample_name = SAMPLE_NAME

    print("\n" + "="*60)
    print(Fore.CYAN + "üß™ EVALUACI√ìN FINAL EN TEST SET" + Style.RESET_ALL)
    print("="*60)

    # Evaluar en test
    print(Fore.YELLOW + "üìä Calculando m√©tricas en test set..." + Style.RESET_ALL)
    test_results = model.evaluate(test_dataset, verbose=1)

    test_loss = test_results[0]
    test_accuracy = test_results[1]
    test_recall = test_results[3] if len(test_results) > 3 else 0.0

    print(f"\n{Fore.GREEN}üìà M√©tricas del Test Set:{Style.RESET_ALL}")
    print(f"   ‚Ä¢ Test Loss: {test_loss:.4f}")
    print(f"   ‚Ä¢ Test Accuracy: {test_accuracy:.4f}")
    print(f"   ‚Ä¢ Test Recall: {test_recall:.4f}")

    # Predecir en test
    print(f"\n{Fore.YELLOW}üîÆ Generando predicciones...{Style.RESET_ALL}")
    y_pred_test = model.predict(test_dataset, verbose=1)
    y_pred_test_classes = np.argmax(y_pred_test, axis=1)

    # Identificar clases presentes en test
    unique_test_classes = np.unique(test_labels)
    print(f"\n{Fore.BLUE}üè∑Ô∏è  Clases presentes en test set: {unique_test_classes}{Style.RESET_ALL}")

    # Verificar si todas las clases est√°n presentes
    missing_classes = set(range(NUM_CLASSES)) - set(unique_test_classes)
    if missing_classes:
        missing_class_names = [CLASS_NAMES[i] for i in missing_classes]
        print(f"{Fore.YELLOW}‚ö†Ô∏è  Clases ausentes en test: {missing_class_names}{Style.RESET_ALL}")

    # Classification report
    print("\n" + "="*60)
    print(Fore.CYAN + "üìä CLASSIFICATION REPORT" + Style.RESET_ALL)
    print("="*60)

    # Especificar solo las clases presentes (recomendado para muestras peque√±as)
    if len(unique_test_classes) < NUM_CLASSES:
        # Usar solo nombres de clases presentes
        target_names_present = [CLASS_NAMES[i] for i in unique_test_classes]
        classification_rep = classification_report(
            test_labels, y_pred_test_classes,
            labels=unique_test_classes,
            target_names=target_names_present,
            digits=4
        )
        print(classification_rep)
        print(f"\n{Fore.YELLOW}‚ö†Ô∏è  Nota: Solo se muestran las {len(unique_test_classes)} clases presentes en el test set.{Style.RESET_ALL}")
    else:
        # Todas las clases presentes, usar reporte completo
        classification_rep = classification_report(
            test_labels, y_pred_test_classes,
            target_names=CLASS_NAMES,
            digits=4
        )
        print(classification_rep)

    if confusion_matrix_viz:
        # Matriz de confusi√≥n
        print(f"\n{Fore.YELLOW}üìà Generando matriz de confusi√≥n...{Style.RESET_ALL}")
        cm = confusion_matrix(test_labels, y_pred_test_classes, labels=unique_test_classes)

        # Usar solo nombres de clases presentes para los ejes
        axis_labels = [CLASS_NAMES[i] for i in unique_test_classes]

        # Crear directorio de modelos si no existe
        os.makedirs(MODELS_PATH, exist_ok=True)

        # Nombre descriptivo para la matriz de confusi√≥n
        confusion_matrix_filename = f'{MODELS_PATH}/confusion_matrix_{model_name}_{sample_name}.png'

        # Crear la matriz de confusi√≥n
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=axis_labels, yticklabels=axis_labels,
                    cbar_kws={'label': 'Count'})
        plt.title(f'Matriz de Confusi√≥n - Test Set\n{model_name} - {sample_name}',
                fontsize=16, fontweight='bold')
        plt.ylabel('Etiqueta Real', fontsize=12)
        plt.xlabel('Predicci√≥n', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(confusion_matrix_filename, dpi=300, bbox_inches='tight')
        plt.close()  # Cerrar la figura para liberar memoria

        print(f"{Fore.GREEN}üíæ Matriz de confusi√≥n guardada: {confusion_matrix_filename}{Style.RESET_ALL}")

    # ==========================================
    # AN√ÅLISIS DE FALSOS NEGATIVOS
    # ==========================================
    fn_metrics = {}
    disease_metrics = {}

    if false_negatives_analysis:
        print(f"\n{Fore.MAGENTA}üîç Ejecutando an√°lisis de falsos negativos...{Style.RESET_ALL}")

        # An√°lisis de falsos negativos por clase
        fn_metrics = analyze_false_negatives(
            test_labels=test_labels,
            y_pred_test_classes=y_pred_test_classes,
            verbose=True
        )

        # An√°lisis de disease recall (detecci√≥n binaria de enfermedades)
        disease_metrics = analyze_disease_recall(
            test_labels=test_labels,
            y_pred_test_classes=y_pred_test_classes,
            verbose=True
        )

    # ==========================================
    # GUARDAR RESULTADOS EN MLFLOW
    # ==========================================

    # Par√°metros base
    params = dict(
        context="evaluate",
        training_set_size=SAMPLE_NAME,
        test_img_count=len(test_labels),
        model_name=model_name,
        classes_in_test=len(unique_test_classes),
        total_classes=NUM_CLASSES,
    )

    # M√©tricas base del modelo
    base_metrics = dict(
        test_loss=test_loss,
        test_accuracy=test_accuracy,
        test_recall=test_recall
    )

    # Combinar TODAS las m√©tricas para MLflow (solo las que existen)
    all_metrics = {
        **base_metrics,               # M√©tricas b√°sicas de evaluaci√≥n
        **fn_metrics,                # An√°lisis de falsos negativos (vac√≠o si no se ejecut√≥)
        **disease_metrics,           # M√©tricas de disease recall (vac√≠o si no se ejecut√≥)
    }

    print(f"\n{Fore.CYAN}üì§ Subiendo m√©tricas a MLflow...{Style.RESET_ALL}")
    print(f"   ‚Ä¢ Par√°metros: {len(params)} items")
    print(f"   ‚Ä¢ M√©tricas totales: {len(all_metrics)} items")
    print(f"     - Evaluaci√≥n b√°sica: {len(base_metrics)}")
    if fn_metrics:
        print(f"     - Falsos negativos: {len(fn_metrics)}")
    if disease_metrics:
        print(f"     - Disease detection: {len(disease_metrics)}")

    # Guardar resultados (esto incluir√° TODAS las m√©tricas en MLflow)
    save_results(params=params, metrics=all_metrics)

    print("\n" + "="*60)
    print(Fore.GREEN + "‚úÖ EVALUACI√ìN COMPLETADA EXITOSAMENTE" + Style.RESET_ALL)
    print("="*60)
    print(f"{Fore.GREEN}üìã Resumen:{Style.RESET_ALL}")
    print(f"   ‚Ä¢ Im√°genes evaluadas: {len(test_labels)}")
    print(f"   ‚Ä¢ Clases evaluadas: {len(unique_test_classes)}/{NUM_CLASSES}")
    print(f"   ‚Ä¢ Accuracy final: {test_accuracy:.4f}")
    print(f"   ‚Ä¢ Recall final: {test_recall:.4f}")

    # Solo mostrar m√©tricas si se calcularon
    if disease_metrics and 'disease_recall' in disease_metrics:
        print(f"   ‚Ä¢ Disease Recall: {disease_metrics['disease_recall']:.4f}")
    if fn_metrics and 'total_false_negatives' in fn_metrics:
        print(f"   ‚Ä¢ Falsos Negativos totales: {fn_metrics['total_false_negatives']}")
        print(f"   ‚Ä¢ Tasa FN global: {fn_metrics.get('overall_false_negative_rate', 0):.1f}%")

    # Informaci√≥n sobre archivos generados
    files_generated = [confusion_matrix_filename]

    print(f"   ‚Ä¢ Archivos generados: {len(files_generated)}")
    for i, file_path in enumerate(files_generated, 1):
        print(f"     {i}. {file_path}")

    print(f"   ‚Ä¢ Total m√©tricas en MLflow: {len(all_metrics)}")

    # Retornar m√©tricas combinadas
    return {
        "test_loss": test_loss,
        "test_accuracy": test_accuracy,
        "test_recall": test_recall,
        "disease_recall": disease_metrics.get('disease_recall', None),
        "classes_evaluated": len(unique_test_classes),
        "total_images": len(test_labels),
        "confusion_matrix_path": confusion_matrix_filename,
        "total_false_negatives": fn_metrics.get('total_false_negatives', None),
        "false_negative_rate": fn_metrics.get('overall_false_negative_rate', None),
        "mlflow_metrics_count": len(all_metrics),
        "files_generated": files_generated,
    }

def pred(img_source=None) -> dict:
    """
    Predicci√≥n flexible que acepta:
    - Ruta de archivo (str): '/path/to/image.jpg'
    - Bytes (bytes): contenido de imagen
    - Base64 (str): string base64 codificado
    """
    print(Fore.MAGENTA + "\nüîé Empezando predicci√≥n... üîé" + Style.RESET_ALL)

    # Intentar cargar modelo existente
    model = get_cached_model()

    if model is None:
        print(Fore.YELLOW + "‚ö†Ô∏è  No se pudo cargar modelo existente" + Style.RESET_ALL)
        print(Fore.BLUE + "üîÑ Para hacer predicciones, necesitas entrenar un modelo primero" + Style.RESET_ALL)
        print(Fore.CYAN + "üí° Ejecuta: make run_train" + Style.RESET_ALL)
        raise ValueError("No hay modelo disponible para predicci√≥n. Entrena un modelo primero con 'make run_train'")

    print(Fore.GREEN + "‚úÖ Modelo cargado exitosamente" + Style.RESET_ALL)

    if img_source is None:
        img_source = input("Ingresa la ruta de la imagen: ").strip()

    # MANEJO ROBUSTO DE DIFERENTES INPUTS
    try:
        if isinstance(img_source, bytes):
            # Caso 1: Bytes directos (desde UploadFile)
            img = Image.open(BytesIO(img_source))

        elif isinstance(img_source, str):
            # Caso 2: String - puede ser ruta O base64

            # Detectar si es base64
            if img_source.startswith('data:image'):
                # Formato: data:image/jpeg;base64,/9j/4AA...
                img_source = img_source.split(',')[1]

            # Intentar decodificar base64
            try:
                img_data = base64.b64decode(img_source)
                img = Image.open(BytesIO(img_data))
            except:
                # Si falla, es una ruta de archivo
                img = Image.open(img_source)
        else:
            raise ValueError(f"Tipo de input no soportado: {type(img_source)}")

    except Exception as e:
        raise ValueError(f"Error al cargar imagen: {str(e)}")

    # Preprocesar
    img = img.resize((224, 224)).convert('RGB')
    img_array = np.array(img) / 255.0
    img_array = np.expand_dims(img_array, 0)

    # Predecir
    predictions = model.predict(img_array)
    predicted_class = np.argmax(predictions)
    probability = float(np.max(predictions))

    print(Fore.GREEN + f"\n‚úÖ Predicci√≥n: {CLASS_NAMES[predicted_class]} ({probability:.4f})" + Style.RESET_ALL)

    return {
        "class_name": CLASS_NAMES[predicted_class],
        "probability": probability,
        "raw": {
            CLASS_NAMES[i]: float(predictions[0][i])
            for i in range(len(CLASS_NAMES))
        }
    }

@mlflow_run
def upload_model_to_gcs(model_version: str = None, dry_run: bool = False):
    """
    Sube el √∫ltimo modelo entrenado a Google Cloud Storage

    Args:
        model_version: Versi√≥n espec√≠fica del modelo (si None, usa timestamp)
        dry_run: Si es True, solo simula la subida sin ejecutarla

    Returns:
        dict: Informaci√≥n sobre la subida
    """
    print(Fore.MAGENTA + "\n‚òÅÔ∏è  Subiendo modelo a GCS... ‚òÅÔ∏è" + Style.RESET_ALL)

    try:
        # Ejecutar subida
        result = upload_latest_model_to_gcs(
            model_version=model_version,
            include_metadata=True,
            dry_run=dry_run
        )

        if not dry_run:
            # Guardar informaci√≥n de la subida en MLflow
            save_results(
                params={
                    "context": "gcs_upload",
                    "model_version": result["model_version"],
                    "gcs_bucket": "configured",
                    "include_metadata": True
                },
                metrics={
                    "model_size_mb": result["model_size_mb"],
                    "metadata_fields": result["metadata_fields"],
                    "upload_success": 1 if result["success"] else 0
                }
            )

            print(f"\n{Fore.GREEN}‚úÖ Modelo subido exitosamente a GCS{Style.RESET_ALL}")
            print(f"   ‚Ä¢ Versi√≥n: {result['model_version']}")
            print(f"   ‚Ä¢ Tama√±o: {result['model_size_mb']:.2f} MB")
            print(f"   ‚Ä¢ Ruta GCS: {result['gcs_paths']['model']}")

        return result

    except Exception as e:
        print(f"{Fore.RED}‚ùå Error subiendo modelo: {e}{Style.RESET_ALL}")
        raise

def list_gcs_models(limit: int = 10):
    """
    Lista los modelos disponibles en GCS

    Args:
        limit: N√∫mero m√°ximo de modelos a mostrar
    """
    print(Fore.CYAN + "\nüìã Modelos en Google Cloud Storage" + Style.RESET_ALL)
    print("="*60)

    try:
        models = list_models_in_gcs(limit=limit)

        if not models:
            print(f"{Fore.YELLOW}üìÇ No se encontraron modelos en GCS{Style.RESET_ALL}")
            return []

        print(f"\nüìä Encontrados {len(models)} modelos:")
        for i, model in enumerate(models, 1):
            print(f"\n{i}. {model['name']}")
            print(f"   üìè Tama√±o: {model['size_mb']:.2f} MB")
            print(f"   üìÖ Creado: {model['created'].strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"   üîó Ruta: {model['gcs_path']}")

        return models

    except Exception as e:
        print(f"{Fore.RED}‚ùå Error listando modelos: {e}{Style.RESET_ALL}")
        return []
